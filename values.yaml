# Ollama Helm Dependency settings
ollama:
  ollama:
    gpu:
      # -- Enable GPU integration
      enabled: false

      # -- GPU type: 'nvidia' or 'amd'
      # If 'ollama.gpu.enabled', default value is nvidia
      # If set to 'amd', this will add 'rocm' suffix to image tag if 'image.tag' is not override
      # This is due cause AMD and CPU/CUDA are different images
      type: 'nvidia'

      # -- Specify the number of GPU
      # If you use MIG section below then this parameter is ignored
      number: 1

      # -- only for nvidia cards; change to (example) 'nvidia.com/mig-1g.10gb' to use MIG slice
      nvidiaResource: "nvidia.com/gpu"
      # nvidiaResource: "nvidia.com/mig-1g.10gb" # example
      # If you want to use more than one NVIDIA MIG you can use the following syntax (then nvidiaResource is ignored and only the configuration in the following MIG section is used)

      mig:
        # -- Enable multiple mig devices
        # If enabled you will have to specify the mig devices
        # If enabled is set to false this section is ignored
        enabled: false

        # -- Specify the mig devices and the corresponding number
        devices: {}
            #        1g.10gb: 1
            #        3g.40gb: 1

    models:
      # -- List of models to pull at container startup
      # The more you add, the longer the container will take to start if models are not present
      # pull:
      #  - llama2
      #  - mistral
      pull: [llama3.2:1b]

      # -- List of models to load in memory at container startup
      # run:
      #  - llama2
      #  - mistral
      run: [llama3.2:1b]
